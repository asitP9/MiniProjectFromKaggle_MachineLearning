{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2acc3090",
   "metadata": {},
   "source": [
    "1. The sentiment dataset consists of 3000 sentences which comes from reviews from \"imdb.com\", \"amazon.com\" and \"yelp.com\". Each sentence is labelled according to whether it comes from a positive review or a negative review.\n",
    "2. The dataset consists of 3000 sentences, each labelled '1' (if it comes from a positive review) or '-1'(if it comes from a negative review). \n",
    "3. we will use logistic regression to learn a classifier from this data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c6795",
   "metadata": {},
   "source": [
    "## Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4520b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1421f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/full_set.txt\") as f:\n",
    "    content=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c4af612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading and trailing whitespaces\n",
    "content=[x.strip() for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71fa959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the sentences from the labels\n",
    "sentences=[x.split(\"\\t\")[0] for x in content]\n",
    "labels=[x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "# transform the label from '0 versus 1' to '-1 versus 1'\n",
    "y=np.array(labels, dtype='int8')\n",
    "y=2*y-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2fc744",
   "metadata": {},
   "source": [
    "Preprocessing the text data\n",
    "* to transform this prediction problem into a linear classification, we will need to preprocess the text data. we will do 4 transformation:\n",
    "    * Remove punctuations and numbers\n",
    "    * transform all words to lowercase\n",
    "    * remove stop words\n",
    "    * convert the sentences into vectors, using a bag-of-word representation\n",
    "\n",
    "* we begin with first 2 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18bfe366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full_remove takes a string x and a list of characters \"removal list\" and returns with all the characters \n",
    "# in removal list replaced by \" \"\n",
    "\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x=x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "digits=[str(x) for x in range(10)]\n",
    "\n",
    "digit_less=[full_remove(x, digits) for x in sentences]      # remove digits\n",
    "punc_less=[full_remove(x, list(string.punctuation)) for x in digit_less] # remove punctuation\n",
    "\n",
    "\n",
    "sents_lower=[x.lower() for x in punc_less] # make everything lowercase\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d70f0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me plug in here in us unless go by converter',\n",
       " 'good case excellent value',\n",
       " 'great for jawbone',\n",
       " 'tied charger for conversations lasting more than minutes major problems',\n",
       " 'mic is great',\n",
       " 'have jiggle plug get line up right get decent volume',\n",
       " 'if you have several dozen or several hundred contacts then imagine fun sending each them one by one',\n",
       " 'if you are razr owner you must have this',\n",
       " 'needless say wasted my money',\n",
       " 'what waste money and time']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words- stop words are the words that are filtered out because they are believed to contain no useful information\n",
    "# for the task at hand.\n",
    "# These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they' and prepositions such as 'to' and 'from'.\n",
    "\n",
    "stop_set=set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "# remove stop words\n",
    "sents_split=[x.split() for x in sents_lower]\n",
    "sents_processed=[\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]\n",
    "\n",
    "# let us look at the sentences\n",
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d16e70",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "* In order to use the linear classifiers on our dataset, we need to transform our textual data into numeric data. The \n",
    "classical way to do this is known as \"the bag of words\" representation.\n",
    "* In this representation, each word is thought of as corressponding to a number in \"{1, 2, ....V}\", where V is the size\n",
    "of our vocabulary. and each sentence is represented as a 'V' dimensional vector x, where xi is the no of times that word i \n",
    "occurs in the sentence.\n",
    "*To do this transformation, we will make use of the \"CountVectorizer\" class in scikit-learn. we will cap the no of features \n",
    "at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. \n",
    "This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n",
    "* finally, we will also append a '1' to the end of each vector to allow our linear classifier to learn a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad47e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i m train_inds  2500\n",
      "Train data:  (2500, 4500)\n",
      "Test data:  (500, 4500)\n"
     ]
    }
   ],
   "source": [
    "# transform to bag of words representation\n",
    "vectorizer=CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=4500)\n",
    "data_features=vectorizer.fit_transform(sents_processed)\n",
    "\n",
    "# append 1 to the end of each vector\n",
    "data_mat=data_features.toarray()\n",
    "\n",
    "# Training/Test split : we split the data into a training set of 2500 sentences\n",
    "# and a test set of 500 sentences(of which 250 are positive and 250 are negative)\n",
    "# split the data into testng and training set\n",
    "np.random.seed(0)\n",
    "test_inds=np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds=list(set(range(len(labels)))-set(test_inds))\n",
    "train_data=data_mat[train_inds,]\n",
    "train_labels=y[train_inds]\n",
    "\n",
    "test_data=data_mat[test_inds,]\n",
    "test_labels=y[test_inds]\n",
    "\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f49fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
